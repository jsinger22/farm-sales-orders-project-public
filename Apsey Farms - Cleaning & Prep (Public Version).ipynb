{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d98656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e6d245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in orders data, single_items_crosswalk from Data Collection notebook, \n",
    "# and bundles_crosswalk (manually created csv from associated bundled product info)\n",
    "orders = pd.read_csv('/Users/josh/Documents/Data Science/Apsey Farms/orders_export_1.csv')\n",
    "single_items_crosswalk = pd.read_csv('/Users/josh/Documents/Data Science/Apsey Farms/single_items_crosswalk.csv')\n",
    "bundles_crosswalk = pd.read_csv('/Users/josh/Documents/Data Science/Apsey Farms/bundle_crosswalk.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd144343",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 100\n",
    "orders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2455ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orders.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0453d378",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_items_crosswalk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb556b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bundles_crosswalk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16fab9f",
   "metadata": {},
   "source": [
    "## Create a single product crosswalk dataframe\n",
    "Ultimately, we want one product crosswalk that we'll use to cross-reference order data. So, let's get bundles_crosswalk into the same format as single_items_crosswalk and combine the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21840b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restructure bundles_crosswalk dataframe\n",
    "bundles_crosswalk.drop('Contents', axis=1, inplace=True)\n",
    "bundles_crosswalk['product_type'] = 'Bundle'\n",
    "bundles_crosswalk.rename(columns={'Bundle Name':'item_name', 'Beef':'quantity_beef_lb', \n",
    "                                 'Pork':'quantity_pork_lb', 'Chicken':'quantity_chicken_lb', \n",
    "                                 'Turkey':'quantity_turkey_lb', 'Total Weight':'total_quantity_lb',\n",
    "                                'Enterprise':'enterprise'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1c206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bundles_crosswalk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317ed68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine crosswalk dataframes\n",
    "product_crosswalk = pd.concat([single_items_crosswalk,bundles_crosswalk])\n",
    "product_crosswalk.reset_index(drop=True, inplace=True)\n",
    "product_crosswalk['Lineitem name'] = product_crosswalk['item_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d4bd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_crosswalk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f368b3",
   "metadata": {},
   "source": [
    "## Preliminary cleaning of orders data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fab371",
   "metadata": {},
   "source": [
    "Let's start by dropping columns we don't need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3537dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bcefbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orders.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46caca5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# find columns with >50% of values missing\n",
    "cols_over_half_missing_values = list(orders.columns[orders.isnull().sum()/len(orders) > 0.5])\n",
    "cols_over_half_missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29cad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now let's keep some of these columns we might use in our analysis and drop the rest \n",
    "# we'll also drop additional columns that won't be useful (e.g. 'Currency' only has one value of USD)\n",
    "drop_cols = cols_over_half_missing_values\n",
    "remove_from_drop_cols = ['Discount Code','Shipping Method','Shipping City','Shipping Zip',\n",
    "                                                'Shipping Province','Notes','Tags','Shipping Province Name']\n",
    "add_to_drop_cols = ['Currency','Billing Street','Billing Address1','Billing Country','Payment Reference','Vendor',\n",
    "                    'Outstanding Balance','Source']\n",
    "\n",
    "for col in remove_from_drop_cols:\n",
    "    drop_cols.remove(col)\n",
    "    \n",
    "for col in add_to_drop_cols:\n",
    "    drop_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72de701",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703d72f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns from orders dataset\n",
    "orders.drop(drop_cols,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad212522",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4d1b22",
   "metadata": {},
   "source": [
    "Let's convert the 'Created at' column to datetime, then create a new column with just the month and year of each order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f32f219",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders['Created at'] = pd.to_datetime(orders['Created at'], utc=True).dt.tz_convert('US/Eastern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4116553",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders['Created at'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1d9549",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders['order_month'] = orders['Created at'].dt.strftime('%Y-%m')\n",
    "orders['order_month'] = pd.to_datetime(orders['order_month'])\n",
    "orders['order_month'] = orders['order_month'].dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08618399",
   "metadata": {},
   "source": [
    "Now let's explore some of our features to determine if there is additional cleaning we can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4477e946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique line items (products)\n",
    "len(orders['Lineitem name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347e39bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders['Lineitem name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e6642f",
   "metadata": {},
   "source": [
    "Looks like we have some suspicious \"products\" e.g. UPS Shipping. Since \"legit\" products most likely contain certain words e.g. \"beef\", \"pork\", \"chicken\", let's filter those out of the 'Lineitem name' columns and investigate suspicious further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33736161",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary version\n",
    "suspicious_items_dict = {}\n",
    "for item in orders['Lineitem name']:\n",
    "    if not any(value in item.lower() for value in ('beef','pork','chicken','turkey','steak','bundle','box','bone','egg','steer','rib')):\n",
    "        if item in suspicious_items_dict:\n",
    "            suspicious_items_dict[item] += 1\n",
    "        elif item not in suspicious_items_dict:\n",
    "            suspicious_items_dict[item] = 1\n",
    "\n",
    "# list version\n",
    "suspicious_items_list = []\n",
    "for item in orders['Lineitem name']:\n",
    "    if not any(value in item.lower() for value in ('beef','pork','chicken','turkey','steak','bundle','box','bone','egg','steer','rib')):\n",
    "        suspicious_items_list.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6b9ccd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "suspicious_items_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d6c7a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert dict to df\n",
    "suspicious_items_df = pd.DataFrame.from_dict(suspicious_items_dict, orient='index').reset_index()\n",
    "suspicious_items_df = suspicious_items_df.rename(columns={'index':'Lineitem name', 0:'count'})\n",
    "suspicious_items_df.sort_values('count', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f50fe30",
   "metadata": {},
   "source": [
    "Now that we've narrowed our list of suspicious products down, let's investigate them further to determine if they should be removed from our dataset for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0949f348",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_suspicious_lines = len(orders[orders['Lineitem name'].isin(suspicious_items_list)])\n",
    "num_total_lines = len(orders)\n",
    "print('Count of suspicious line items:', num_suspicious_lines)\n",
    "print('Suspicious line items as a % of total line items:', round(num_suspicious_lines/num_total_lines*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c267797",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orders[orders['Lineitem name'].isin(suspicious_items_list)][['Lineitem name','Lineitem quantity','Total','Subtotal','Discount Amount',\n",
    "                                                       'Lineitem price','Created at']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d3079a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# total amount paid from products\n",
    "orders[orders['Lineitem name'].isin(suspicious_items_list)]['Total'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e6e52f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lineitem names for suspicious_items with total = $0\n",
    "orders[(orders['Lineitem name'].isin(suspicious_items_list)) & (orders['Total']==0)]['Lineitem name']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798a4eb9",
   "metadata": {},
   "source": [
    "Since the majority of the suspicious_items have a total amount paid of $0 and suspicious_items only make up <3\\% of the total number of line items, let's drop these rows from our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7410720",
   "metadata": {},
   "outputs": [],
   "source": [
    "suspicious_items_index = list(orders[orders['Lineitem name'].isin(suspicious_items_list)].index)\n",
    "orders = orders.drop(suspicious_items_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9cbda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that rows were dropped\n",
    "orders[orders['Lineitem name'].isin(suspicious_items_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05ba68a",
   "metadata": {},
   "source": [
    "We know that Apsey Farms sometimes gives away products for free for promotions, gifts, etc. We won't consider these giveaways to be \"true\" sales/orders, so let's drop them from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc53da0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(orders[orders['Total']==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea6d80a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check for orders with $0 total\n",
    "orders[orders['Total']==0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff382628",
   "metadata": {},
   "outputs": [],
   "source": [
    "free_giveaways_index = list(orders[orders['Total']==0].index)\n",
    "orders = orders.drop(free_giveaways_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c96f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that rows were dropped\n",
    "orders[orders['Total']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fe9698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of line items (our true orders/sales) we're left with for analysis\n",
    "len(orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c72f07",
   "metadata": {},
   "source": [
    "## Modify product crosswalk to be a comprehensive single source of truth\n",
    "#### Add items from orders data not already in crosswalk to crosswalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25fdf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique item names from orders, convert to df, and add to crosswalk\n",
    "unique_lineitem_names = pd.DataFrame(orders['Lineitem name'].unique(), columns=['Lineitem name'])\n",
    "product_crosswalk_full = pd.concat([product_crosswalk,unique_lineitem_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1e83cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "product_crosswalk_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929c156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "product_crosswalk_full.duplicated(['Lineitem name']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ca29f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates, keeping the first entry as it contains the associated feature values\n",
    "product_crosswalk_full = product_crosswalk_full.drop_duplicates(['Lineitem name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4988f007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm duplicates were dropped (370 - 72 = 298)\n",
    "product_crosswalk_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b40b87",
   "metadata": {},
   "source": [
    "#### Use Fuzzy Matching to impute values for missing items. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f9247a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b453bcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's separate the items with known values from those with missing values\n",
    "known_items = product_crosswalk_full[product_crosswalk_full['item_name'].notna()]['item_name']\n",
    "missing_items = product_crosswalk_full[product_crosswalk_full['item_name'].isna()]['Lineitem name']\n",
    "print(known_items[:5])\n",
    "print('\\n')\n",
    "print(missing_items[:5])\n",
    "print('\\n')\n",
    "print('Total # known items:',len(known_items))\n",
    "print('Total # missing items:',len(missing_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9912b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use fuzzywuzzy.process.extractOne() to find the top known item match for each missing item\n",
    "fuzzy_top_choice = {}\n",
    "for item in missing_items:\n",
    "    fuzzy_top_choice[item] = process.extractOne(item, known_items)\n",
    "fuzzy_top_choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cc1741",
   "metadata": {},
   "source": [
    "For the most part, it appears that choices with a score of 90 or greater (the second value in the choice tuple) are a good match with the missing item. However, it looks like fuzzywuzzy didn't do as great when the score is less than 90. So, we'll impute the values for missing items using choices with a score greater than or equal to 90, and use a different fuzzy function (fuzz.token_set_ratio) for cases where the score was less than 90 to pull out the top 3 matches so that we can manually choose the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb184594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use score_cutoff parameter to filter for good matches using process.extractOne\n",
    "fuzzy_top_choice = {}\n",
    "for item in missing_items:\n",
    "    fuzzy_top_choice[item] = process.extractOne(item, known_items, score_cutoff=89)\n",
    "fuzzy_top_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3e6157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll use these libraries to extract keys with top values from dictionaries\n",
    "import heapq\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a01306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through fuzzy_top_choice dictionary and create separate dataframes for good and bad matches\n",
    "\n",
    "fuzzy_below_90 = pd.DataFrame()\n",
    "good_matches = {}\n",
    "\n",
    "for key, value in fuzzy_top_choice.items():\n",
    "    # if score from fuzzy_top_choice was <90, find top three scores using fuzzy token_set_ratio \n",
    "    # and add to fuzzy_below_90 df\n",
    "    if value == None:\n",
    "        ratios = {}\n",
    "        for item in known_items:\n",
    "            ratios[item] = fuzz.token_set_ratio(key, item)\n",
    "        # get top 3 choices, returned as list of tuples where 1st element in tuple is the choice name and 2nd is fuzzy score\n",
    "        top_3_items = heapq.nlargest(3, ratios.items(), key=itemgetter(1))\n",
    "        # select only the choice name, not the score\n",
    "        top_3_choices = [i[0] for i in top_3_items]\n",
    "        top_3_choices_dict = {key: top_3_choices}\n",
    "        top_3_choices_df = pd.DataFrame.from_dict(top_3_choices_dict,orient='index',columns=['choice_1','choice_2',\n",
    "                                                                                             'choice_3'])\n",
    "        top_3_choices_df = top_3_choices_df.reset_index().rename(columns={'index':'Lineitem name'})\n",
    "        fuzzy_below_90 = pd.concat([fuzzy_below_90,top_3_choices_df])\n",
    "    \n",
    "    # if score from fuzzy_top_choice was >=90, add choice to good_matches_df\n",
    "    else:\n",
    "        good_matches[key] = value\n",
    "        good_matches_df = pd.DataFrame.from_dict(good_matches, orient='index', columns=['choice','score','index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d815a4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of \"good\" matches:',len(good_matches))\n",
    "print('Number of \"bad\" matches:',len(fuzzy_below_90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe84ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_matches_df = good_matches_df.reset_index().drop(['score','index'],axis=1)\n",
    "good_matches_df = good_matches_df.rename(columns={'level_0':'Lineitem name','choice':'item_name'})\n",
    "good_matches_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dfc7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_below_90 = fuzzy_below_90.reset_index().drop('index',axis=1)\n",
    "fuzzy_below_90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace72e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export fuzzy_below_90 to csv so that we can manually label the best match\n",
    "fuzzy_below_90.to_csv('fuzzy_below_90.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31dc753",
   "metadata": {},
   "source": [
    "Let's impute values for good matches using the matching items in product_crosswalk_full, then we'll add the good matches back to our product crosswalk. Note that we'll need to drop the old, non-imputed items from product crosswalk; we'll do this at the end, after we've added values for good and bad matches to the product crosswalk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6b5edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_matches_impute = pd.merge(left=good_matches_df, right=product_crosswalk_full, how='left', on='item_name')\n",
    "good_matches_impute = good_matches_impute.drop('Lineitem name_y',axis=1).rename(columns={'Lineitem name_x':'Lineitem name'})\n",
    "good_matches_impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b4457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_crosswalk_final = pd.concat([product_crosswalk_full,good_matches_impute])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad699f85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "product_crosswalk_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c0778a",
   "metadata": {},
   "source": [
    "We manually labeled the best choice for our fuzzy_below_90 matches. Let's pull the data back in and add it to our product crosswalk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0448c203",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_below_90_labeled = pd.read_csv('/Users/josh/Documents/Data Science/Apsey Farms/fuzzy_below_90_labeled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31c3839",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_below_90_labeled.drop(['Unnamed: 0','choice_1','choice_2','choice_3'],axis=1,inplace=True)\n",
    "fuzzy_below_90_labeled.rename(columns={'final_choice':'item_name'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d57423",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_below_90_labeled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fecf6de",
   "metadata": {},
   "source": [
    "Now we'll impute values for our fuzzy_below_90 matches using the matching items in product_crosswalk_full, then add those matches back to our product crosswalk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87fabd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_matches_impute = pd.merge(left=fuzzy_below_90_labeled, right=product_crosswalk_full, how='left', on='item_name')\n",
    "fuzzy_matches_impute = fuzzy_matches_impute.drop('Lineitem name_y',axis=1).rename(columns={'Lineitem name_x':'Lineitem name'})\n",
    "fuzzy_matches_impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec243ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_matches_impute['product_type'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37b2a1c",
   "metadata": {},
   "source": [
    "Looks like we have a number of items that did not match a previously defined product, so we'll have to fill in values for these. Notes:\n",
    " * for \"bulk\" items, the 'Lineitem quantity' field in the orders data indicates the weight in pounds, rather than quantity of items ordered. We'll leave the quantity columns blank for now, and impute those values when we merge the product crosswalk back to our orders data.\n",
    " * items labeled \"eggs\" will not have an associated weight, rather we measure quantity by the dozen. So, we'll also leave the quantity columns blank for those items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52bdc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_matches_impute.to_csv('fuzzy_matches_impute.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d548e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_matches_impute_labeled = pd.read_csv('/Users/josh/Documents/Data Science/Apsey Farms/fuzzy_matches_impute_labeled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23fe8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_matches_impute_labeled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0c264f",
   "metadata": {},
   "source": [
    "Let's merge these items into our product crosswalk, then drop the old, non-imputed/duplicate items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744bd32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_crosswalk_final = pd.concat([product_crosswalk_final,fuzzy_matches_impute_labeled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b749bd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "product_crosswalk_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c239342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_crosswalk_final.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b25c6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_crosswalk_final.drop('index',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fa409f",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_crosswalk_final['item_name'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57948ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_items = list(product_crosswalk_final[product_crosswalk_final['item_name'].isnull()].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083df074",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_crosswalk_final = product_crosswalk_final.drop(null_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a87f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that null_items were dropped\n",
    "product_crosswalk_final['item_name'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0db96d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that product crosswalk contains 298 items\n",
    "len(product_crosswalk_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf21eab7",
   "metadata": {},
   "source": [
    "#### Use weights specified in item names to update item attributes\n",
    "For example, 'Lineitem Name' = 'Ground Beef - 6 lbs' would previously have been matched with the standard ground beef item and assumed its standard quantity of 1 lb; however, 'quantity_beef_lb' for this item should instead be 6 (lbs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff7826f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# find all Lineitem name's containing 'lb' or 'oz'\n",
    "# only include product_type = 'Single item' since we've manually populated values for some bundles and don't want\n",
    "# values for bulk items\n",
    "single_products = product_crosswalk_final[product_crosswalk_final['product_type']=='Single item']\n",
    "products_with_quantity = single_products[(single_products['Lineitem name'].str.contains('lb')) | (single_products['Lineitem name'].str.contains('oz'))]\n",
    "products_with_quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef60d1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03049dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract quantity\n",
    "# pattern = r\"[.+]\\s[.+]\\s(?P<quantity>[.+])\"\n",
    "# pattern_2 = r\"(\\d*\\.?\\d+[+]?[\\s]?[-]?[\\s]?[\\d*\\.?\\d+]?[\\.\\d+]?)\"\n",
    "pattern = r\"(?P<quantity>\\d*\\.?\\d+[+]?[\\s]?[-]?[\\s]?[\\d*]?[\\.]?[\\d*]?)\\s?(?P<measure>lbs?|lb?|oz?)\"\n",
    "quantity_extract = products_with_quantity['Lineitem name'].str.extract(pattern, flags=re.I)\n",
    "quantity_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffbbe98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add extract to products_with_quantity dataframe by joining on the index\n",
    "products_quant_extracted = pd.merge(left=products_with_quantity, right=quantity_extract, how='left', left_index=True, right_index=True)\n",
    "products_quant_extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480de9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with missing quantity values\n",
    "drop_rows = list(products_quant_extracted[products_quant_extracted['quantity'].isna()].index)\n",
    "products_quant_extracted = products_quant_extracted.drop(drop_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9569c91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that rows were dropped\n",
    "print(products_quant_extracted['quantity'].isna().sum())\n",
    "print(len(products_quant_extracted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad5c270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn ranges of values in the quantity column into a single value by taking the average of the range min and max\n",
    "# then create a new column with this value\n",
    "def find_avg_quantity(value):\n",
    "    quants = str(value).split('-')\n",
    "    if len(quants) == 1:\n",
    "        return value\n",
    "    elif len(quants) == 2:\n",
    "        return (float(quants[0])+float(quants[1]))/2\n",
    "    \n",
    "products_quant_extracted['quantity_avg'] = products_quant_extracted['quantity'].apply(find_avg_quantity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc647aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "products_quant_extracted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a9e629",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "products_quant_extracted['quantity_avg'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacc3e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove '+' from quantity_avg\n",
    "# products_quant_extracted['quantity_avg'] = products_quant_extracted['quantity_avg'].str.replace('+','')\n",
    "\n",
    "def remove_plus_sign(value):\n",
    "    if '+' in str(value):\n",
    "        return value.replace('+','')\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "products_quant_extracted['quantity_avg'] = products_quant_extracted['quantity_avg'].apply(remove_plus_sign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2188914b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "products_quant_extracted.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220d3437",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_quant_extracted['measure'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4818eb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert quantity_avg's in oz to lbs\n",
    "products_quant_extracted['quantity_avg'] = products_quant_extracted['quantity_avg'].astype('float')\n",
    "products_quant_extracted['quantity_avg_lb'] = np.where(products_quant_extracted['measure']=='oz', \n",
    "                                         products_quant_extracted['quantity_avg']/16, products_quant_extracted['quantity_avg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe4d6e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "products_quant_extracted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea86b4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use quantity_avg to update quantity values\n",
    "# excluding Turkey since we updated those manually previously\n",
    "products_quant_extracted['quantity_beef_lb'] = np.where(products_quant_extracted['enterprise']=='Beef',\n",
    "                                                     products_quant_extracted['quantity_avg_lb'],0)\n",
    "products_quant_extracted['quantity_pork_lb'] = np.where(products_quant_extracted['enterprise']=='Pork',\n",
    "                                                     products_quant_extracted['quantity_avg_lb'],0)\n",
    "products_quant_extracted['quantity_chicken_lb'] = np.where(products_quant_extracted['enterprise']=='Chicken',\n",
    "                                                     products_quant_extracted['quantity_avg_lb'],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b4e608",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_quant_extracted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b02d3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reset total_quantity_lb\n",
    "products_quant_extracted['total_quantity_lb'] = products_quant_extracted['quantity_beef_lb']+products_quant_extracted['quantity_pork_lb']+products_quant_extracted['quantity_chicken_lb']+products_quant_extracted['quantity_turkey_lb']\n",
    "products_quant_extracted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8f5174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unecessary columns\n",
    "products_quant_extracted = products_quant_extracted.drop(['quantity','measure','quantity_avg','quantity_avg_lb'],\n",
    "                                                        axis=1)\n",
    "products_quant_extracted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b633716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_quant_extracted.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e9af50",
   "metadata": {},
   "source": [
    "Now that we've updated the quantity attributes for items that contained a quantity in their name, let's merge these items back into our product crosswalk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf09f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_crosswalk_final = pd.concat([product_crosswalk_final,products_quant_extracted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e215911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_crosswalk_final.duplicated(['Lineitem name']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310be0fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we want to keep the last duplicate row, since products_quant_extracted was added to the end of the \n",
    "# product_crosswalk_final df\n",
    "product_crosswalk_final.drop_duplicates(['Lineitem name'],keep='last',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e599ba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_crosswalk_final.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373117b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_crosswalk_final.drop('index',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea83c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_crosswalk_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a4301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_crosswalk_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9daa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_crosswalk_final['product_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcc1d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_crosswalk_final['enterprise'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4c278c",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_crosswalk_final.to_csv('product_crosswalk_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f544ce",
   "metadata": {},
   "source": [
    "## Add features to Orders data that we'll use in our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7450fc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_clean = orders.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a699022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge orders with product crosswalk\n",
    "orders_clean = pd.merge(left=orders_clean, right=product_crosswalk_final, how='left', on='Lineitem name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97d9e4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orders_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3b1567",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orders_clean[orders_clean['total_quantity_lb'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aa7984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new columns with the total item weight and weight per enterprise\n",
    "# note: doesn't apply to bulk items and eggs\n",
    "orders_clean['total_item_weight'] = orders_clean['Lineitem quantity'] * orders_clean['total_quantity_lb']\n",
    "orders_clean['item_weight_beef'] = orders_clean['Lineitem quantity'] * orders_clean['quantity_beef_lb']\n",
    "orders_clean['item_weight_pork'] = orders_clean['Lineitem quantity'] * orders_clean['quantity_pork_lb']\n",
    "orders_clean['item_weight_chicken'] = orders_clean['Lineitem quantity'] * orders_clean['quantity_chicken_lb']\n",
    "orders_clean['item_weight_turkey'] = orders_clean['Lineitem quantity'] * orders_clean['quantity_turkey_lb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2ccdbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orders_clean.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e0cc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_clean.to_csv('orders_clean.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
